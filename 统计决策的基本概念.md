统计决策与贝叶斯估计
================================
- 统计决策理论，是参数估计（点估计，区间估计）的一种推广。

一、统计决策的基本概念
--------------------------------
### 1. 统计判决问题的三个要素
- 为了估计一个未知参数，需要给出一个合适的统计量，该估计量也称为该统计问题的解。
- 一般地说，一个统计问题的解就是所谓的统计决策函数。

为了明确决策函数这一重要概念，下面介绍构成一个统计决策问题的基本要素作一介绍：

#### 1) 样本空间和分布族
- 样本所有可能值所组成的集合称为样本空间。
- 由于$X_i$的分布函数为$F(x_i|\theta),i=1,2,...,n$,
则$(X_1,X_2,...,X_n)^T$的联合分布函数为：
$$F(x_1,...,x_n|\theta)=\prod_{i=1}^{n}F(x_i|\theta),\theta\in\Theta.$$
称$\{\prod_{i=1}^{n}F(x_i|\theta),\theta\in\Theta\}$
为样本$(X_1,X_2,...,X_n)$的概率分布族，简称分布族。

#### 2) 行动空间（或成判决空间）
对参数$\theta$的点估计，一个具体的估计值就是一个回答。
在统计决策中，每个具体的回答称为一个决策，全部决策组成决策空间，决策空间至少包含两个决策。

#### 3) 损失函数
- 统计决策的一个基本观点和假设是，每一个决策有一定的后果，决策不同，后果各异。
- 统计决策理论一个基本思想是把这个后果，以数量的形式表现出来，其方法就是引入一个依赖于参数$\theta$和决策$d$的二元实值非负函数
$$L(\theta,d)\geq0.$$
称为损失函数。
- 对于不同的统计问题，可选不同的损失函数：

(1)线性损失函数：
$$L(\theta,d)=\begin{cases}
k_0(\theta-d), & d \leq \theta\\
k_1(d-\theta), & d \gt \theta\\
\end{cases}$$
当$k_0=k_1=1$时，得到绝对值损失函数：
$$L(\theta,d)=|\theta-d|$$

(2)平方损失函数：
$$L(\theta,d)=(\theta-d)^2$$

(3)凸损失函数(*略*)

(4)多元二次损失函数(*略*)

### 2. 统计决策函数及其风险函数
#### 1) 统计决策函数
定义在样本空间上，取之于决策空间内的函数$d(x)$。简称决策函数

例如：正态分布$\mu$的一个区间估计为：
$$d(x)=(\overline{x}-\mu_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}},\overline{x}+\mu_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}})$$
则$d(x)$就是一个决策函数

#### 2) 风险函数
损失函数$L(\theta,d(x))$是基于样本$\boldsymbol{x}$的，故引入风险函数来比较决策函数的优劣。

$$R(\theta,d)=E_\theta[L(\theta,d(\boldsymbol{X}))]=E_\theta[L(\theta,d(X_1,X_2,...,X_n))]$$
称$\theta$的函数$R(\theta,d)$为决策函数$d(\boldsymbol{X})$的风险函数。

对于给定的损失函数来说，可以在决策空间中找出一个一致最小风险决策函数，或称一致最有决策函数。
但是当选择的损失函数改变时，原来的决策函数的一致最优性可能就不具备了。


二、统计决策中的常用分布族
--------------------------------
除了以下的分布族，还有之前介绍过的$\chi^2$分布族$\{\chi^2:n\geq1\}$，$t$分布族$\{t(n):n\geq1\}$，$F$分布族$\{F(n_1,n_2):n_1\geq1:n_2\geq1\}$
### 1.Gamma分布族
### 2.贝塔分布族
三、贝叶斯估计
--------------------------------
在一个统计问题中，可供选择的决策函数往往很多，自然希望寻找到是风险最小的，然而在这种意义下的最有决策函数往往是不存在的。
这是因为风险函数$R(\theta,d)$是既依赖于参数$\theta$又依赖于决策函数$d$的二元函数。
而贝叶斯方法通过引进先验分布把两个风险函数的点点比较转化为一个用整体指标的比较来代替，从而可以决定优劣。

以下两个小节是贝叶斯的基础，前者还包括似然函数，而后者讲的是如何根据总体分布来选择先验分布。
### 先验分布和后验分布
### 共轭先验分布
### 贝叶斯风险
由于$\boldsymbol{X}$和$\pi(\theta)$均是随机变量，则求风险函数，需要经过两次求期望。
第一次为对$\theta$的厚颜分布求期望，第二次则关于样本$\boldsymbol{X}$的边缘分布求期望。
求得的风险函数称为决策函数$d$在给定先验分布$\pi(\theta)$下的贝叶斯风险，简称$d$的贝叶斯风险。

当总体$\boldsymbol{X}$和\theta都是连续型随机变量是，有
$$R_B(d)=\int_Hm(\boldsymbol{x})\{\int_\Theta L(\theta,d(\boldsymbol{x}))h(\theta|\boldsymbol{x})d\theta\}$$
其中,$H$为样本空间，$m(\boldsymbol{x})$为样本的边际密度，$\Theta$为参数空间，$L(\theta,d(\boldsymbol{x}))$为损失函数，$h(\theta|\boldsymbol{x})$为后验密度。

当总体$\boldsymbol{X}$和\theta都是离散型随机变量是，有
$$R_B(d)=\sum_xm(\boldsymbol{x})\{\sum_\theta L(\theta,d(\boldsymbol{x})h(\theta,\boldsymbol{x}))\}$$

由于$R_B(d)$已不依赖于参数$\theta$而仅依赖于决策函数$d(\boldsymbol{X})$，
因此，以贝叶斯风险的大小作为衡量决策函数优劣的标准是合理的。

### 贝叶斯估计
#### 1.贝叶斯点估计
记为参数$\theta$的贝叶斯估计量$d^*(\boldsymbol{X})$就是使贝叶斯风险$R_B(d)$达到最小的决策函数。

注意，贝叶斯股计量使依赖于先验分布$\pi(\theta)$的，即对于不同的先验函数，$\theta$的贝叶斯估计量使不同的。

在常用损失函数下，贝叶斯估计有如下几个结论：

四、minimax估计
--------------------------------
五、经验贝叶斯估计
--------------------------------
